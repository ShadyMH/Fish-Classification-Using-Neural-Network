# -*- coding: utf-8 -*-
"""cnn__3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14a4svgSH52q_WM51j3WBYJ-__n_Qbrbw

### Uploading dataset
"""

!pip install kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d crowww/a-large-scale-fish-dataset

!mkdir -p /content/a-large-scale-fish-dataset
!unzip /content/a-large-scale-fish-dataset.zip -d /content/a-large-scale-fish-dataset

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""### Import necessary Libraries"""

# to prevent unnecessary warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# TensorFlow and tf.keras
import tensorflow as tf

from pathlib import Path

#import useful module for keras library
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import regularizers

# get modules from sklearn library
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

#import libraries
import matplotlib.pyplot as plt
import seaborn as sns
import random

"""### Load the dataset and Explore"""

file = Path("a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset") #dataset location path
File_Path = list(file.glob(r"**/*.png"))
Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],File_Path))

File_Path = pd.Series(File_Path).astype(str)
Labels = pd.Series(Labels)
df = pd.concat([File_Path,Labels],axis=1)
df.columns = ['image', 'label']
# Drop all the images that ends with (GT)

df = df[df["label"].apply(lambda x: x[-2:] != "GT")].reset_index(drop=True)

df.head() #get first 5 rows of the dataset

# Display 15 picture of the dataset with their labels
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),
                        subplot_kw={'xticks': [], 'yticks': []})

for i, ax ,in enumerate(axes.flat):
    ax.imshow(plt.imread(df.image[i]))
    ax.set_title(df.label[i])

plt.tight_layout()
plt.show()

"""### Split dataset for validation"""

#split remaining data into train and test sets
train_set, test_set = train_test_split(df, test_size = 0.3, random_state = 24)

#split the train set into train and evaluation set

train_set, val_set = train_test_split(train_set, test_size= 0.2, random_state = 24)

print(train_set.shape)
print(val_set.shape)
print(test_set.shape)

"""### Perform Preprocessing on the Images"""

img_gen = ImageDataGenerator(rescale=1/255)
batch = 64
target = (224,224)

# img_gen cannot take in an array, so ensure the data that is been passed is a dataframe
train = img_gen.flow_from_dataframe(dataframe = train_set,
    x_col = 'image', #name of the column containing the image in the train set
    y_col ='label', #name of column containing the target in the train set
    target_size = target,
    color_mode = 'rgb',
    class_mode = 'categorical',#the class mode here and that for the model_loss(when using sequential model)
                                    #should be the same
    batch_size = batch,
    shuffle = True #shuffle the given data
)

val = img_gen.flow_from_dataframe(dataframe = val_set,
    x_col = 'image', #name of the column containing the image in the validation set
    y_col ='label', #name of column containing the target in the validation set set
    target_size = target,
    color_mode ='rgb',
    class_mode ='categorical',
    batch_size = batch,
    shuffle = True #set to false so as not to shuffle the given data
)

test = img_gen.flow_from_dataframe(dataframe = test_set,
    x_col = 'image', #name of the column containing the image in the test set
    y_col ='label', #name of column containing the target in the test set
    target_size = target,
    color_mode ='rgb',
    class_mode ='categorical',
    batch_size = batch,
    shuffle = False # shuffle the given data
)

"""### Building CNN

"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    tf.keras.layers.Dense(9, activation='softmax')
])

# View model layers summary
model.summary()

# Plot model architecture
tf.keras.utils.plot_model(model,
                          show_shapes=True,
                          show_dtype=True,
                          show_layer_names=True)

# compile model
model.compile(optimizer='adam', # optimize the model with adam optimizer
              loss="categorical_crossentropy",
              metrics=['accuracy']) #to get accuracy of the model in each run

"""### Fit and train the model"""

history = model.fit(train, #fit the model on the training set
                    validation_data = val, #add the validation set to evaluate the performance in each run
                    epochs = 20, #train in 5 epochs
                    verbose = 1)

epochs = range(1, len(acc) + 1)  # Generate integer values for epochs

plt.figure(figsize=(8, 8))  # Set figure size for the plot generated
plt.subplot(2, 1, 1)  # A sup plot with 2 rows and 1 column

plt.plot(epochs, acc, label='Training Accuracy')  # Plot accuracy curve for each train run
plt.plot(epochs, val_acc, label='Validation Accuracy')  # Plot accuracy curve for each validation run

plt.legend(loc='lower right')
plt.ylabel('Accuracy')  # Label name for y-axis
plt.title('Training and Validation Accuracy')  # Set title for the plot
plt.xticks(epochs)  # Set integer values for x-axis ticks corresponding to each epoch
plt.xlabel('Epochs')  # Label name for x-axis
plt.ylim([min(plt.ylim()), 1])  # Set limit for y-axis
plt.show()

epochs = range(1, len(loss) + 1)  # Generate integer values for epochs

plt.figure(figsize=(8, 8))  # Set figure size for the plot generated
plt.subplot(2, 1, 1)  # A sup plot with 2 rows and 1 column

plt.plot(epochs, loss, label='Training Loss')  # Plot loss curve for each train run
plt.plot(epochs, val_loss, label='Validation Loss')  # Plot loss curve for each validation run

plt.legend(loc='upper right')
plt.ylabel('Loss')  # Label name for y-axis
plt.title('Training and Validation Loss')  # Set title for the plot
plt.xticks(epochs)  # Set integer values for x-axis ticks corresponding to each epoch
plt.xlabel('Epochs')  # Label name for x-axis
plt.ylim([0, max(max(loss), max(val_loss))])  # Set limit for y-axis
plt.show()

"""### Make prediction"""

# Predict the label of the test_images
pred = model.predict(test)
pred = np.argmax(pred,axis = 1) # pick the class with highest probability
# sequential model predicts by given probability for each of the classes
#np.argmax is called on the prediction to choose the class with the highest probability

# Map the label
labels = (train.class_indices)
labels = dict((v,k) for k,v in labels.items())
pred2 = [labels[k] for k in pred]

from sklearn.metrics import classification_report, confusion_matrix # import metrics for evaluation

y_test = test_set.label # set y_test to the expected output

print(classification_report(y_test, pred2)) # print the classification report

"""### Confusion Matrix"""

import seaborn as sb
from matplotlib.ticker import FuncFormatter
# compare with true labels
cfm = confusion_matrix(y_test, pred2, normalize='true')
# Get unique class labels from the DataFrame
class_labels = df['label'].unique()
# plot size
fig, ax = plt.subplots(figsize=(18,18))
# print confusion matrix
s = sb.heatmap(cfm,
                annot=True,
                cmap=['#ff0000', '#09AA00'],
                center=0.8,
                fmt='.1%',
                linewidths=.5,
                cbar_kws={'format': FuncFormatter(lambda x, pos: '{:.0%}'.format(x))}, #'label': 'Percentage'
                linecolor='white',
                ax=ax)
# set labels
s.set(xlabel='Predict', ylabel='True')
s.set(title='Confusion Matrix')
s.set(xticklabels=class_labels, yticklabels=class_labels)

# Display 15 picture of the dataset with their labels
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),
                        subplot_kw={'xticks': [], 'yticks': []})

color = "blue" if pred2[i] == test_set.label.iloc[i] else "red"
for i, ax ,in enumerate(axes.flat):
    ax.imshow(plt.imread(test_set.image.iloc[i]))
    ax.set_title(f"True: {test_set.label.iloc[i]}\nPredicted: {pred2[i]}",color=color)

plt.subplots_adjust(hspace = 0.3)
plt.suptitle("Model predictions (blue: correct, red: incorrect)",y=0.98)
plt.tight_layout()
plt.show()